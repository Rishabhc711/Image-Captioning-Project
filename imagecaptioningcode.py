# -*- coding: utf-8 -*-
"""ImageCaptioningcode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nyoXY52qo3myAiSxuk7whTs8cYSX9L_O

# Image Captioning Project

This project involved the use of a modified **pre-trained ResNet50 model** to map english words to the input imageâ€™s features to produce a caption through text generators (of GloVe word embedding).

**Concepts Used:** Image Processing, Word Embeddings, GAN modelling. 

*   Trained on 8kFlickr dataset. 
*   Coded using keras with tensorflow backend.
*   ResNet50 model is used to extract features from the images.
*   BLEU score as its evaluation metric.
"""

# Importing the Required Libraries

import numpy as np
import matplotlib.pyplot as plt
from keras.models import Model, load_model
from keras.preprocessing import image
import time
import pickle
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import *
import pandas as pd

# Mounting Google Drive

from google.colab import drive
drive.mount('/content/drive')

"""### Data Preprocessing"""

dataset = pd.read_csv('/content/drive/My Drive/AI/Projects/Image captioning/Dataset/FlickrText/New folder/Flickr8k.token.txt',delimiter='\t',header=None)

print(dataset.shape)

dataset.head()

print(dataset[0][3])
print(dataset[1][3])

# Libraries to be used for data cleaning

import re
import nltk


corpus = []
for i in range(0, dataset.shape[0]):
    cap = re.sub('[^a-zA-Z]', ' ', dataset[1][i])  #Removed all other characters except alphabets
    cap = cap.lower()                              #Converted to lower case
    cap = cap.split()                               
    cap=[word for word in cap if len(word)>1]      #Removed single letter words
    cap = ' '.join(cap)                            # Joined with spaces
    cap= '<startseq> '+cap+' <endseq>'
    corpus.append(cap)                             #list of captions

for i in range(50): print(corpus[i:i+1])
print(len(corpus))

i=0
while i < dataset.shape[0]:
  dataset[0][i]=dataset[0][i].split('.')[0]        #Removed characters after '.'
  i=i+1

print(dataset.head)

type(dataset)

# Joins the Images to its captions using a dictionary 

from collections import defaultdict
new_dict= defaultdict(lambda : [])          #Created a dictionary with image ids as key and captions as value
i=0
while i <len(corpus):
  # print(dataset[0][i])
  new_dict[dataset[0][i]].append(corpus[i])
  i=i+1

print(len(new_dict))

print(new_dict['2513260012_03d33305cf'])

all_vocab = []              #A list of all the words in the captions

for key in new_dict.keys():
    [ all_vocab.append(i) for des in new_dict[key] for i in des.split()]

print("total words appearing : " , len(all_vocab))

"""Creating a word counter for the corpus"""

from collections import Counter

counter = Counter(all_vocab)

dic_ = dict(counter)

sorted_dic = sorted(dic_.items(), key = lambda x: x[1], reverse=True)   #List with words and their corresponding frequency

print(len(sorted_dic))

for i in range(len(sorted_dic)):print(sorted_dic[i])

# we decide upon a threshold value which helps in selecting the words that occur more than others in the corpus
# Here we choose a threshold of 10, so that words that occur more than 10 times in the entire corpus are chosen

threshold_value = 10

d = [(x) for x in sorted_dic if x[1]>threshold_value]

len(d)

all_vocab = [x[0] for x in d]                       #Updating all_vocab

len(all_vocab)

f = open('new_dict.txt', 'w')
f.write(str(new_dict))
f.close()

"""#### We read the Training and testing image captions dataset and append it all  in a list """

with open('/content/drive/My Drive/AI/Projects/Image captioning/Dataset/FlickrText/New folder/Flickr_8k.trainImages.txt') as f:
  train=f.read()

print(train)

train = [e[:-4] for e in train.split('\n')[:-1]]                                #train image ids

print(train)

with open('/content/drive/My Drive/AI/Projects/Image captioning/Dataset/FlickrText/New folder/Flickr_8k.testImages.txt')as f:
  test=f.read()

test = [e[:-4] for e in test.split('\n')[:-1]]                                    #test image ids

train_descriptions = {}                                                           #dictionary with keys as image ids and values as corresponding captions
for t in train:
    train_descriptions[t] = []
    for cap in new_dict[t]:
        train_descriptions[t].append(cap)

i=0
for keys,values in train_descriptions.items():
 i=i+1
 
 print(keys)
 print(values)
 if i>50:break

"""### Importing and Loading the model"""

from keras.applications.resnet50 import ResNet50, preprocess_input

model = ResNet50(weights = 'imagenet', input_shape = (224,224,3))

model.summary()

model_new = Model(inputs = model.input, outputs =  model.layers[-2].output)

"""#### Image Preprocessing
We take the train & test image datasets and preprocess them.
"""

# Function to preprocess images

def preprocess_image(img):
    img = image.load_img(img, target_size=(224,224))
    img = image.img_to_array(img)
    img = preprocess_input(img)
    img = np.expand_dims(img, axis = 0)

    return img

# Function for obtaining feature vectors (encodings) from images

def encode_image(img):
    img = preprocess_image(img)
    fea_vec = model_new.predict(img)
    fea_vec = fea_vec.reshape(fea_vec.shape[1], )
    return fea_vec

images='/content/drive/My Drive/AI/Projects/Image captioning/Dataset/Flicker8k_Dataset/'

# Obtaining Feature vector from training images
 
start = time.time()

encoding_train = {}                #Dictionary with keys as image ids and values as encoding corresponding to the images                        

for ix, img in enumerate(train):
    
    img = images+train[ix]+".jpg"
    
    p = encode_image(img)
    
    encoding_train[ img[len(images):] ] = p
    
    
    if ix%100 == 0:
        print("Encoding image :" + str(ix))
    
print("Time taken in sec - " + time.time() - start)

# Obtaining Feature vector from test images

start = time.time()

encoding_test = {}

for ix, img in enumerate(test):
    
    img = images+test[ix]+".jpg"
    
    p = encode_image(img)
    
    encoding_test[ img[len(images):] ] = p
    
    
    if ix%100 == 0:
        print("Encoding image :" + str(ix))
    
print("Time taken in sec - " + time.time() - start)

# saving features to disk

with open("./encoded_train_images.pkl", 'wb') as f:
    pickle.dump(encoding_train, f )

with open("./encoded_test_images.pkl", 'wb') as f:
    pickle.dump(encoding_test, f )

with open("/content/drive/My Drive/AI/Projects/Image captioning/Dataset/Pkl/encoded_train_images.pkl", 'rb') as f:
    encoding_train = pickle.load(f)

with open("/content/drive/My Drive/AI/Projects/Image captioning/Dataset/Pkl/encoded_test_images.pkl", 'rb') as f:
    encoding_test = pickle.load(f)

"""**Captions**"""

word_to_idx = {}
idx_to_word = {}

ix = 1

for e in all_vocab:
    #print(ix,e)
    word_to_idx[e] = ix
    idx_to_word[ix] = e
    ix +=1

i=0
for keys,values in word_to_idx.items():
 i=i+1
 
 print(keys,values)
 
 if i>50:break

i=0
for keys,values in idx_to_word.items():
 i=i+1
 
 print(keys,values)
 
 if i>50:break

for i in range(51):print(all_vocab[i])

len(all_vocab)

vocab_size = len(idx_to_word) + 1
print(vocab_size)

all_caption_len = []

for key in train_descriptions.keys():
    for cap in train_descriptions[key]:
        all_caption_len.append(len(cap.split()))

print(len(all_caption_len))
print(all_caption_len[:50])

max_len = max(all_caption_len)
print(max_len)

"""### Generator Function
The Data Generator maps the Image encodings with the training captions for the training of the model
"""

def data_generator(train_descriptions, encoding_train, word_to_idx, max_len,  num_photos_per_batch ):
    X1, X2, y = [], [], []
    
    n=0
    cnt = 0
    all_items = list(train_descriptions.keys())
    
    while True:
      n+=1
      # print(cnt)
      key = all_items[cnt]
      desc_list = train_descriptions[key]              
      cnt+=1
      cnt= (cnt%len(all_items))


      photo = encoding_train[key+'.jpg']          #feature vector
            #print(photo.shape)
      for desc in desc_list:                       #desc : iterates through the 5 captions
        seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]                
                
        for i in range(1, len(seq)): 
            in_seq = seq[0:i]
            out_seq = seq[i]
            
            in_seq = pad_sequences( [in_seq], maxlen=max_len, value= 0, padding='post')[0]
        
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            
            X1.append(photo)
            X2.append(in_seq)
            y.append(out_seq)

      if n == num_photos_per_batch:
          yield [np.array(X1), np.array(X2)] , np.array(y)
          X1, X2, y = [], [], []
          n = 0

for i in data_generator(train_descriptions, encoding_train, word_to_idx, max_len, 3):
    X, y = i
    print(X[0].shape) 
    print(X[1].shape)
    print(y.shape)
    break

"""###  Word Embedding
Of GloVe embeddings
"""

embeddings = {}

with open('/content/drive/My Drive/AI/Projects/Image captioning/Dataset/FlickrText/glove.6B.50d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coeffs = np.array(values[1:], dtype="float32")
        
        embeddings[word] = coeffs

i=0
for keys,values in embeddings.items():
 i=i+1
 
 print(keys,values)
 
 if i>10:break

print(len(embeddings))

def getOutputEmbeddings():

    emb_dim = 50
    embedding_matrix_output = np.zeros((vocab_size, emb_dim ))
    
    for word, idx in word_to_idx.items():
        
        emb_vec = embeddings.get(word)
        
        if emb_vec is not None:
            embedding_matrix_output[idx] = emb_vec
            
    return embedding_matrix_output

embedding_output = getOutputEmbeddings()

embedding_output.shape

print(embedding_output[5:7])

"""### Model Architecture
Development and stacking of the model's layers
"""

input_img_fea = Input(shape=(2048,))
inp_img1 = Dropout(0.3)(input_img_fea)
inp_img2 = Dense(256, activation='relu')(inp_img1)

print(inp_img2.shape)

input_cap = Input(shape=(max_len,))
inp_cap1 = Embedding(input_dim= vocab_size, output_dim=50, mask_zero=True)(input_cap)
#print(inp_cap1)
inp_cap2 = Dropout(0.3)(inp_cap1)
inp_cap3 = LSTM(256)(inp_cap2)

print(inp_cap3.shape)

decoder1 = add([inp_img2, inp_cap3])
print(decoder1.shape)
decoder2 = Dense(256, activation='relu')(decoder1)
output = Dense(vocab_size, activation='softmax')(decoder2)


model = Model(inputs = [input_img_fea, input_cap]  , outputs =  output )

model.summary()

model.layers[2].set_weights([embedding_output])
model.layers[2].trainable = False

model.compile(loss="categorical_crossentropy", optimizer='adam')

"""### Model Training"""

epochs = 10
number_photos_per_batch = 3
steps = len(train_descriptions)//number_photos_per_batch

mytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)

model.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)
#model.save("best_model1.h5")

model.save( filepath="/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model.h5")

model.save_weights("/content/drive/My Drive/Weights/weights1.h5")

epochs = 20
number_photos_per_batch = 3
steps = len(train_descriptions)//number_photos_per_batch

mytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)

model.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)
model.save(filepath="/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model1.h5")

epochs = 10
number_photos_per_batch = 3
steps = len(train_descriptions)//number_photos_per_batch

mytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)

model.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)
#model.save("best_model1.h5")

model.save_weights("/content/drive/My Drive/AI/Projects/Image captioning/Weights/weights2.h5")

"""### Predictor Function
This function takes an image and predicts the corresponding caption for it by pasiing it into the model
"""

def predict(photo_enc,model):
    in_text = "<startseq>"
    
    for i in range(max_len):
        sequence = [word_to_idx[word] for word in in_text.split() if word in word_to_idx]
        #print(sequence)
        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')
        
        y_pred = model.predict([photo_enc, sequence])
        y_pred = np.argmax(y_pred)
        word = idx_to_word[y_pred]
        
        in_text += " "+word
        
        if word == '<endseq>':
            break
        
        
    final_caption = in_text.split()
    final_caption = final_caption[1:-1]
    final_caption = " ".join(final_caption)
    return final_caption

#in_text = "<startseq>"
#for i in range(max_len):
        #sequence = [word_to_idx[word] for word in in_text.split() if word in word_to_idx]
        #print(sequence)
        #sequence = pad_sequences([sequence], maxlen=max_len, padding='post')
#print(sequence)

#print(word_to_idx['<startseq>'])

"""Predicting from best  3 different models"""

model=load_model('/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model6.h5')
model1=load_model('/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model350.h5')
model2=load_model('/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model7.h5')

for rn in range(50):
 img_id = list(encoding_test.keys())[rn]

 photo_enc = encoding_test[img_id].reshape((1,2048))
 pred = predict(photo_enc,model1)
 pred1 = predict(photo_enc,model)
 pred2 = predict(photo_enc,model2)

 print('1 '+pred)
 print('2 '+pred1)
 print('3 '+pred2)

 path = images + img_id
 img = plt.imread(path)
 plt.imshow(img)
 plt.show()

#model=load_model('/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model6.h5')

#epochs = 5
#number_photos_per_batch = 3
#steps = len(train_descriptions)//number_photos_per_batch

#mytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)

#model.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)

#epochs = 150
#number_photos_per_batch = 3
#steps = len(train_descriptions)//number_photos_per_batch

#mytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)

#model.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)
#model.save_weights("/content/drive/My Drive/AI/Projects/Image captioning/Weights/weights"+str(20)+ ".h5")

m#odel.save( filepath="/content/drive/My Drive/AI/Projects/Image captioning/Model/best_model350.h5")

#sequence = pad_sequences([1], maxlen=35, padding='post')
#y_pred = model.predict([photo_enc, sequence])

#epochs = 20
#number_photos_per_batch = 3
#steps = len(train_descriptions)//number_photos_per_batch

#mytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)

#model.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)

#model.save("best_model5.h5")

#model.save_weights("/content/drive/My Drive/Weights/weights5.h5")

#model.load_weights('/content/drive/My Drive/Weights/weights1.h5')

#model.compile(optimizer='adam',loss='categorical_crossentropy')